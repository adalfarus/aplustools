1010010110010101001010101011010
1010101010010101010101010101001

Compressor starts at byte position 61
automatic reordering of chunks & metadata
bytes potential encryption for single chunks
yeah

Make it possible to compress large files as one block.

Currently the block size isn't really treated as the block size and more as a suggestion, we should fix that.
We will have a current_data_buffer in which we write the added file, that will then get chopped up into [chunk_size]
blocks and added to the file.

Add methods for finding closest empty block-range for a specific size. (First, we compress 1 MB then we look up that
so the block size isn't the final block size, but the raw data size, or how much data we feed the compressor)


data_structure:

file_info = {"MyFile.txt": {
                'index': file_index,
                'block_index': len(self.block_offsets),  # Start block index (all in line)
                'start': len(self.current_block),
                'length': len(data),
                'start_block': index
            },
            ...
            }  # Dict are ordered in python 3.7, just use them (lib supports 3.9->3.12)?

block_offsets = [{  # How do we identidy chunks?
    'start': 17_992,
    'length': 1024 * 1024  // 1 mb
}]  # Would be good to have this as a linked list style data structure, but currently these are chunks chunks not file
# Chunks (or should be) so we will have one block_offsets which are the actual blocks and then a file_to_block dict
# that records all chunks a given file is in (in the specific chunk it has an offset and a length) and the next chunk if
# it exists otherwise None

def get_closest_chunk(self, size):
    for offset in self.block_offsets:
        print(offset)
